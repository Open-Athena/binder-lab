{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Analyzing Boltz Prediction Results\n",
        "\n",
        "This notebook demonstrates how to load and analyze Boltz structure predictions using the functions in `binder_lab.common`.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Make sure you have run the Snakemake workflow to generate prediction CSV files:\n",
        "- Individual predictor CSVs: `boltz1_predictions_data.csv`, `boltz2_predictions_data.csv`\n",
        "- Aggregated CSV: `all_predictions_data.csv`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "# Import our custom functions\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from binder_lab.common import (\n",
        "    load_predictions_csv, \n",
        "    load_and_analyze_predictions,\n",
        "    get_structure_info,\n",
        "    get_confidence_summary,\n",
        "    get_plddt_summary\n",
        ")\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Load Prediction Results\n",
        "\n",
        "First, let's load the aggregated CSV file with all predictions from all predictors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Path to your results (adjust as needed)\n",
        "results_dir = Path('../results')  # Adjust this path\n",
        "csv_path = results_dir / 'all_predictions_data.csv'\n",
        "\n",
        "# Check if the file exists\n",
        "if csv_path.exists():\n",
        "    print(f\"Loading predictions from: {csv_path}\")\n",
        "    \n",
        "    # Load with full analysis (structures, confidence, NPZ data)\n",
        "    df = load_and_analyze_predictions(csv_path, base_dir=results_dir)\n",
        "    \n",
        "    print(f\"\\nLoaded {len(df)} predictions\")\n",
        "    print(f\"Columns: {list(df.columns)}\")\n",
        "else:\n",
        "    print(f\"CSV file not found: {csv_path}\")\n",
        "    print(\"Please run the Snakemake workflow first to generate prediction data.\")\n",
        "    # Create dummy data for demonstration\n",
        "    df = pd.DataFrame({\n",
        "        'predictor': ['boltz1', 'boltz2'],\n",
        "        'design_name': ['demo_design', 'demo_design'],\n",
        "        'model_idx': [0, 0],\n",
        "        'note': ['This is demo data - run workflow to get real results']\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Basic Data Overview\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic info about the dataset\n",
        "if len(df) > 0 and 'predictor' in df.columns:\n",
        "    print(\"=== Dataset Overview ===\")\n",
        "    print(f\"Total predictions: {len(df)}\")\n",
        "    print(f\"Predictors: {sorted(df['predictor'].unique())}\")\n",
        "    print(f\"Designs: {sorted(df['design_name'].unique())}\")\n",
        "    \n",
        "    if 'model_idx' in df.columns:\n",
        "        print(f\"Models per design: {df['model_idx'].max() + 1}\")\n",
        "    \n",
        "    # Show first few rows\n",
        "    print(\"\\n=== First Few Rows ===\")\n",
        "    display_cols = ['predictor', 'design_name', 'model_idx']\n",
        "    if 'plddt_mean' in df.columns:\n",
        "        display_cols.extend(['plddt_mean', 'struct_num_residues'])\n",
        "    \n",
        "    available_cols = [col for col in display_cols if col in df.columns]\n",
        "    print(df[available_cols].head())\n",
        "else:\n",
        "    print(\"No prediction data available - please run the workflow first.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Inspect Original Design Data\n",
        "\n",
        "Each row contains the original design information from the YAML file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(df) > 0 and 'design_dict' in df.columns:\n",
        "    # Look at the original design for the first prediction\n",
        "    first_design_dict = df.iloc[0]['design_dict']\n",
        "    original_design = json.loads(first_design_dict)\n",
        "    \n",
        "    print(\"=== Original Design Data ===\")\n",
        "    print(f\"Design name: {original_design.get('name', 'Unknown')}\")\n",
        "    print(f\"Sequences:\")\n",
        "    \n",
        "    for i, seq in enumerate(original_design.get('sequences', [])):\n",
        "        print(f\"  Sequence {i+1}:\")\n",
        "        if 'protein' in seq:\n",
        "            protein = seq['protein']\n",
        "            print(f\"    Type: Protein\")\n",
        "            print(f\"    ID: {protein.get('id')}\")\n",
        "            print(f\"    Length: {len(protein.get('sequence', ''))} residues\")\n",
        "            print(f\"    Sequence: {protein.get('sequence', '')[:50]}...\")\n",
        "            if 'designed' in protein:\n",
        "                designed = protein['designed']\n",
        "                num_designed = designed.count('D')\n",
        "                print(f\"    Designed positions: {num_designed}/{len(designed)}\")\n",
        "        elif 'ligand' in seq:\n",
        "            ligand = seq['ligand']\n",
        "            print(f\"    Type: Ligand\")\n",
        "            print(f\"    ID: {ligand.get('id')}\")\n",
        "            print(f\"    CCD: {ligand.get('ccd')}\")\n",
        "        print()\n",
        "else:\n",
        "    print(\"No design data available.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Structure Analysis\n",
        "\n",
        "If structures were parsed successfully, we can analyze them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(df) > 0 and 'structure' in df.columns:\n",
        "    # Check how many structures were successfully parsed\n",
        "    structures_parsed = df['structure'].notna().sum()\n",
        "    print(f\"=== Structure Analysis ===\")\n",
        "    print(f\"Structures successfully parsed: {structures_parsed}/{len(df)}\")\n",
        "    \n",
        "    if structures_parsed > 0:\n",
        "        # Look at structure info columns\n",
        "        struct_cols = [col for col in df.columns if col.startswith('struct_')]\n",
        "        if struct_cols:\n",
        "            print(f\"\\nStructure info columns: {struct_cols}\")\n",
        "            print(\"\\nStructure summary:\")\n",
        "            display(df[['predictor', 'design_name', 'model_idx'] + struct_cols].head())\n",
        "        \n",
        "        # Get a specific structure object\n",
        "        first_structure = df[df['structure'].notna()].iloc[0]['structure']\n",
        "        if first_structure is not None:\n",
        "            print(f\"\\n=== Example Structure Details ===\")\n",
        "            print(f\"Structure name: {first_structure.name}\")\n",
        "            print(f\"Number of models: {len(first_structure)}\")\n",
        "            \n",
        "            if len(first_structure) > 0:\n",
        "                model = first_structure[0]\n",
        "                print(f\"Number of chains: {len(model)}\")\n",
        "                \n",
        "                for chain in model:\n",
        "                    print(f\"  Chain {chain.name}: {len(chain)} residues\")\n",
        "                    if len(chain) > 0:\n",
        "                        first_res = chain[0]\n",
        "                        last_res = chain[-1]\n",
        "                        print(f\"    Residues: {first_res.name}{first_res.seqid.num} to {last_res.name}{last_res.seqid.num}\")\n",
        "    else:\n",
        "        print(\"No structures were parsed. Check if gemmi is installed and CIF files exist.\")\n",
        "else:\n",
        "    print(\"No structure data available.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. pLDDT Analysis and Visualization\n",
        "\n",
        "Analyze per-residue confidence scores and create visualizations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(df) > 0 and 'plddt_mean' in df.columns:\n",
        "    print(\"=== pLDDT Analysis ===\")\n",
        "    \n",
        "    # Summary statistics\n",
        "    print(f\"Mean pLDDT across all predictions: {df['plddt_mean'].mean():.2f}\")\n",
        "    print(f\"pLDDT range: {df['plddt_mean'].min():.2f} - {df['plddt_mean'].max():.2f}\")\n",
        "    \n",
        "    # Look at pLDDT summary columns\n",
        "    plddt_cols = [col for col in df.columns if col.startswith('plddt_')]\n",
        "    if plddt_cols:\n",
        "        print(f\"\\npLDDT summary columns: {plddt_cols}\")\n",
        "        print(\"\\npLDDT statistics:\")\n",
        "        display(df[['predictor', 'design_name', 'model_idx'] + plddt_cols].head())\n",
        "    \n",
        "    # Create visualization\n",
        "    if len(df) > 1:\n",
        "        print(\"\\n=== Creating Visualizations ===\")\n",
        "        \n",
        "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "        \n",
        "        # 1. pLDDT distribution\n",
        "        axes[0].hist(df['plddt_mean'], bins=20, alpha=0.7, edgecolor='black', color='skyblue')\n",
        "        axes[0].set_xlabel('Mean pLDDT')\n",
        "        axes[0].set_ylabel('Count')\n",
        "        axes[0].set_title('Distribution of Mean pLDDT Scores')\n",
        "        axes[0].axvline(df['plddt_mean'].mean(), color='red', linestyle='--', \n",
        "                       label=f\"Mean: {df['plddt_mean'].mean():.1f}\")\n",
        "        axes[0].legend()\n",
        "        \n",
        "        # 2. pLDDT by predictor (if multiple predictors)\n",
        "        if len(df['predictor'].unique()) > 1:\n",
        "            df.boxplot(column='plddt_mean', by='predictor', ax=axes[1])\n",
        "            axes[1].set_title('pLDDT by Predictor')\n",
        "            axes[1].set_xlabel('Predictor')\n",
        "            axes[1].set_ylabel('Mean pLDDT')\n",
        "        else:\n",
        "            axes[1].text(0.5, 0.5, 'Single predictor\\n(no comparison)', \n",
        "                        ha='center', va='center', transform=axes[1].transAxes)\n",
        "            axes[1].set_title('pLDDT by Predictor')\n",
        "        \n",
        "        # 3. High vs low confidence (if available)\n",
        "        if 'plddt_high_conf' in df.columns and 'plddt_low_conf' in df.columns:\n",
        "            scatter = axes[2].scatter(df['plddt_high_conf'], df['plddt_low_conf'], \n",
        "                                    alpha=0.6, c=df['plddt_mean'], cmap='viridis')\n",
        "            axes[2].set_xlabel('Fraction High Confidence (>90)')\n",
        "            axes[2].set_ylabel('Fraction Low Confidence (<50)')\n",
        "            axes[2].set_title('High vs Low Confidence Residues')\n",
        "            plt.colorbar(scatter, ax=axes[2], label='Mean pLDDT')\n",
        "        else:\n",
        "            axes[2].text(0.5, 0.5, 'Detailed confidence data\\nnot available', \n",
        "                        ha='center', va='center', transform=axes[2].transAxes)\n",
        "            axes[2].set_title('Confidence Analysis')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Comparative analysis if multiple predictors\n",
        "        if len(df['predictor'].unique()) > 1:\n",
        "            print(\"\\n=== Comparative Analysis ===\")\n",
        "            print(\"\\nMean pLDDT by predictor:\")\n",
        "            predictor_stats = df.groupby('predictor')['plddt_mean'].agg(['mean', 'std', 'count'])\n",
        "            display(predictor_stats)\n",
        "            \n",
        "            print(\"\\nMean pLDDT by design:\")\n",
        "            design_stats = df.groupby('design_name')['plddt_mean'].agg(['mean', 'std', 'count'])\n",
        "            display(design_stats)\n",
        "            \n",
        "else:\n",
        "    print(\"No pLDDT data available for analysis.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Next Steps\n",
        "\n",
        "This notebook demonstrates the basic usage of the prediction analysis functions. You can extend this analysis by:\n",
        "\n",
        "1. **Structure Analysis**: Use the `structure` objects with gemmi to analyze specific structural features\n",
        "2. **Custom Metrics**: Calculate domain-specific quality metrics  \n",
        "3. **Comparative Studies**: Compare different designs or predictors systematically\n",
        "4. **Advanced Visualization**: Create structure visualizations using tools like PyMOL or NGLView\n",
        "5. **Export**: Convert structures to other formats (PDB, etc.) for further analysis\n",
        "\n",
        "### Key Functions Demonstrated:\n",
        "\n",
        "- `load_and_analyze_predictions()` - Load CSV and parse all associated files\n",
        "- `load_predictions_csv()` - More control over what gets loaded\n",
        "- Structure objects via gemmi - Access to full structural information\n",
        "- Summary columns - Quick access to key metrics\n",
        "\n",
        "### Example Usage for Advanced Analysis:\n",
        "\n",
        "```python\n",
        "# Load only specific data types\n",
        "df = load_predictions_csv('all_predictions_data.csv', \n",
        "                         parse_structures=True,\n",
        "                         parse_confidence=False, \n",
        "                         parse_npz=True)\n",
        "\n",
        "# Access raw structure data\n",
        "structure = df.iloc[0]['structure']  # gemmi.Structure object\n",
        "plddt_data = df.iloc[0]['plddt']     # dict with numpy arrays\n",
        "\n",
        "# Use structure with gemmi\n",
        "for model in structure:\n",
        "    for chain in model:\n",
        "        for residue in chain:\n",
        "            # Analyze specific residues\n",
        "            pass\n",
        "```\n",
        "\n",
        "The parsed data in the DataFrame gives you access to all the prediction outputs in a convenient, analyzable format.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
